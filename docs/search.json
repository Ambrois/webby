[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Matthew Huang",
    "section": "",
    "text": "Welcome to my website :P\nClick here to download a virus: TODO add virus"
  },
  {
    "objectID": "Projects.html",
    "href": "Projects.html",
    "title": "Projects",
    "section": "",
    "text": "Transformers\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/about/about.html",
    "href": "posts/about/about.html",
    "title": "About",
    "section": "",
    "text": "Close this page right now."
  },
  {
    "objectID": "posts/about/about.html#about-this-blog",
    "href": "posts/about/about.html#about-this-blog",
    "title": "About",
    "section": "",
    "text": "Close this page right now."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Close this page right now."
  },
  {
    "objectID": "about.html#about-this-blog",
    "href": "about.html#about-this-blog",
    "title": "About",
    "section": "",
    "text": "Close this page right now."
  },
  {
    "objectID": "posts/transformers.html",
    "href": "posts/transformers.html",
    "title": "Transformer Notes (Draft)",
    "section": "",
    "text": "Transformer Circuits Thread (transformer-circuits.pub)\n\n\nprivileged basis definition\n“activation”, in the context of activation vs parameter\nhow exactly is position embedded into a vector space (in the space that the key and query vectors are in) - and if it even does encode position (or like subsets of positions within context) - and can this position embedding be more generalized to other methods of storing tokens, instead of just a rly long string - hierarchical - graphical -\n\n\n\nresidual stream: linear combination of inputs and attention layer outputs - it’s entirely linear, and acts as a subspace - attention layers perform projections onto the subspace - due to linearity, at any 2 points, can make a matrix which represents the whole transformation up to that point - ? could this be used in actual transformers to reduce the amount of computation used? - has to be in parts which are linear\n\n\n\n\n\nT(t) : Predicted Logits from input tokens (the Transformer Output) (context x vocab) matrix - “Activation, Privileged Basis”\nt : a vector of tokens (each of which is a one-hot encoded vector) in the context (context x vocab) matrix - “Activation, Privileged Basis”\n\\(x^n\\) : vector of embedding vectors in the residual stream (at layer n) (context x semantic dims) matrix - each vector is the updated semantic representation of a context token after n layers - “Activation, Not Privileged Basis”\n\\(W_E\\) : Embedding Weights, a transformation from one-hot input vectors to embedding vectors (semantic dims x vocab) transformation matrix - input is a vector of tokens (the context), output is a vector of embedding vectors - only takes into account the type of tokens - (frequency?) - Parameter\n\\(W_P\\) : Positional Embedding Weights, a transformation from position in context to embedding vector (semantic dims x context) transformation matrix - input is a vector of tokens (the context), output is a vector of embedding vectors - only takes into account the positions - (time?) - Parameter\n\\(W_U\\) : Unembedding Weights, a transformation from embedding vectors to vocab vectors (logits) (vocab x semantic dims) transformation matrix - (is this the inverse of W_E?) - includes softmax? - Parameter\n\n\n\n\\(H_n\\) : Attention Heads, the set of attention heads at a layer n\n\\(h \\in H_n\\) : An attention head, a function from embedding vectors to embedding vectors\n\\(h(x)\\) : The output of attention head \\(h\\) given some embedding vectors \\(x\\) (context, semantic dims) matrix - a vector of semantic embedding vectors of each token of the context\n\\(\\bf{A}^h\\) : Attention pattern of head h, Not sure what this is, the matrix representing the linear transformation? a transformation from embedded context vectors to embedded context vectors? (context, context) transformation matrix\nFor each head h: this is how you go from input \\(x_i\\) to output \\(h(x)_i\\) Informally: embedding vector -(Value Matrix)-&gt; value vector -(Attention Pattern)-&gt; result vector -(Output Matrix)&gt; output embedding vector Formally: \\(W_V​x_i = v_i\\) , then \\(\\sum_j{ A_{i, j} v_j} = r_i\\) , then \\(W_Or_i=h(x)_i\\)\n\n\n\n\n\n\n“A privileged basis occurs when some aspect of a model’s architecture encourages neural network features to align with basis dimensions”\nConfusion: A privileged basis is when vectors (aka directions) have specific meanings and can’t be rotated without changing those meanings? - but don’t embedding values have meanings assigned to specific directions? unless the meanings are all relative to other vectors - what are the embedding values if their directions don’t have specific meanings - or maybe the directions do have specific meanings, but these meanings don’t have to align to particular neurons\nExamples: “for example (a privileged basis occurs) because of a sparse activation function such as ReLU”\nVectors which do not have a privileged basis: “it doesn’t really make sense to look at the”neurons” (ie. basis dimensions) of activations like the residual stream, keys, queries or values, which don’t have a privileged basis.” - residual stream / embedding - Output of attention head \\(h\\) - Query, key, value and result vectors of attention head \\(h\\) - Output of MLP layer \\(m\\)\nVectors which do have a privileged basis: - Transformer logits - One-hot encoded tokens - Attention pattern of attention head \\(h\\) - Activations of MLP layer \\(m\\)\n\n\n\nwhen combined, is the transformation of the input to output\nvalue matrix is the input token, output is the output token - and when multiplied together, they make up the transformation\nSpeculative what if you find the nearest token (or token sequence) to this final output value vector, to get a description\n\n\n\nFrom a point in embedding space, how to efficiently find a sequence of tokens which approximates that value - maybe given a minimum variance or distance from target - can we even actually represent sequences of tokens as a single point in the embedding space?\nhow to represent an arbitrary sequence of tokens as a single point in an embedding space - what are the scaling difficulties of representing more and more complex stuff in a single embedding space? - relationship of necessary dim size to token sequence length? - like maybe the more tokens in a sequence you’d like to represent, the more embedding dimensions you need to represent the semantic information - maybe instead of using the same embedding dim for everything, use different ones for different “levels” of token sequence length - sort of representing complexity? abstraction?\nconstruct graph relations from - attention to get"
  },
  {
    "objectID": "posts/transformers.html#things-i-dont-get",
    "href": "posts/transformers.html#things-i-dont-get",
    "title": "Transformer Notes (Draft)",
    "section": "",
    "text": "privileged basis definition\n“activation”, in the context of activation vs parameter\nhow exactly is position embedded into a vector space (in the space that the key and query vectors are in) - and if it even does encode position (or like subsets of positions within context) - and can this position embedding be more generalized to other methods of storing tokens, instead of just a rly long string - hierarchical - graphical -"
  },
  {
    "objectID": "posts/transformers.html#notes",
    "href": "posts/transformers.html#notes",
    "title": "Transformer Notes (Draft)",
    "section": "",
    "text": "residual stream: linear combination of inputs and attention layer outputs - it’s entirely linear, and acts as a subspace - attention layers perform projections onto the subspace - due to linearity, at any 2 points, can make a matrix which represents the whole transformation up to that point - ? could this be used in actual transformers to reduce the amount of computation used? - has to be in parts which are linear"
  },
  {
    "objectID": "posts/transformers.html#basic-definitions",
    "href": "posts/transformers.html#basic-definitions",
    "title": "Transformer Notes (Draft)",
    "section": "",
    "text": "T(t) : Predicted Logits from input tokens (the Transformer Output) (context x vocab) matrix - “Activation, Privileged Basis”\nt : a vector of tokens (each of which is a one-hot encoded vector) in the context (context x vocab) matrix - “Activation, Privileged Basis”\n\\(x^n\\) : vector of embedding vectors in the residual stream (at layer n) (context x semantic dims) matrix - each vector is the updated semantic representation of a context token after n layers - “Activation, Not Privileged Basis”\n\\(W_E\\) : Embedding Weights, a transformation from one-hot input vectors to embedding vectors (semantic dims x vocab) transformation matrix - input is a vector of tokens (the context), output is a vector of embedding vectors - only takes into account the type of tokens - (frequency?) - Parameter\n\\(W_P\\) : Positional Embedding Weights, a transformation from position in context to embedding vector (semantic dims x context) transformation matrix - input is a vector of tokens (the context), output is a vector of embedding vectors - only takes into account the positions - (time?) - Parameter\n\\(W_U\\) : Unembedding Weights, a transformation from embedding vectors to vocab vectors (logits) (vocab x semantic dims) transformation matrix - (is this the inverse of W_E?) - includes softmax? - Parameter\n\n\n\n\\(H_n\\) : Attention Heads, the set of attention heads at a layer n\n\\(h \\in H_n\\) : An attention head, a function from embedding vectors to embedding vectors\n\\(h(x)\\) : The output of attention head \\(h\\) given some embedding vectors \\(x\\) (context, semantic dims) matrix - a vector of semantic embedding vectors of each token of the context\n\\(\\bf{A}^h\\) : Attention pattern of head h, Not sure what this is, the matrix representing the linear transformation? a transformation from embedded context vectors to embedded context vectors? (context, context) transformation matrix\nFor each head h: this is how you go from input \\(x_i\\) to output \\(h(x)_i\\) Informally: embedding vector -(Value Matrix)-&gt; value vector -(Attention Pattern)-&gt; result vector -(Output Matrix)&gt; output embedding vector Formally: \\(W_V​x_i = v_i\\) , then \\(\\sum_j{ A_{i, j} v_j} = r_i\\) , then \\(W_Or_i=h(x)_i\\)"
  },
  {
    "objectID": "posts/transformers.html#notes-1",
    "href": "posts/transformers.html#notes-1",
    "title": "Transformer Notes (Draft)",
    "section": "",
    "text": "“A privileged basis occurs when some aspect of a model’s architecture encourages neural network features to align with basis dimensions”\nConfusion: A privileged basis is when vectors (aka directions) have specific meanings and can’t be rotated without changing those meanings? - but don’t embedding values have meanings assigned to specific directions? unless the meanings are all relative to other vectors - what are the embedding values if their directions don’t have specific meanings - or maybe the directions do have specific meanings, but these meanings don’t have to align to particular neurons\nExamples: “for example (a privileged basis occurs) because of a sparse activation function such as ReLU”\nVectors which do not have a privileged basis: “it doesn’t really make sense to look at the”neurons” (ie. basis dimensions) of activations like the residual stream, keys, queries or values, which don’t have a privileged basis.” - residual stream / embedding - Output of attention head \\(h\\) - Query, key, value and result vectors of attention head \\(h\\) - Output of MLP layer \\(m\\)\nVectors which do have a privileged basis: - Transformer logits - One-hot encoded tokens - Attention pattern of attention head \\(h\\) - Activations of MLP layer \\(m\\)\n\n\n\nwhen combined, is the transformation of the input to output\nvalue matrix is the input token, output is the output token - and when multiplied together, they make up the transformation\nSpeculative what if you find the nearest token (or token sequence) to this final output value vector, to get a description\n\n\n\nFrom a point in embedding space, how to efficiently find a sequence of tokens which approximates that value - maybe given a minimum variance or distance from target - can we even actually represent sequences of tokens as a single point in the embedding space?\nhow to represent an arbitrary sequence of tokens as a single point in an embedding space - what are the scaling difficulties of representing more and more complex stuff in a single embedding space? - relationship of necessary dim size to token sequence length? - like maybe the more tokens in a sequence you’d like to represent, the more embedding dimensions you need to represent the semantic information - maybe instead of using the same embedding dim for everything, use different ones for different “levels” of token sequence length - sort of representing complexity? abstraction?\nconstruct graph relations from - attention to get"
  },
  {
    "objectID": "Blog.html",
    "href": "Blog.html",
    "title": "Blog",
    "section": "",
    "text": "Narrative Visualization\n\n\n\n\n\n\n\n\n\n\n\nMatthew Huang\n\n\n\n\n\n\n\n\n\n\n\n\nTransformer Notes (Draft)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/transformers/transformers.html",
    "href": "posts/transformers/transformers.html",
    "title": "Transformer Notes (Draft)",
    "section": "",
    "text": "Transformer Circuits Thread (transformer-circuits.pub)\n\n\nprivileged basis definition\n“activation”, in the context of activation vs parameter\nhow exactly is position embedded into a vector space (in the space that the key and query vectors are in) - and if it even does encode position (or like subsets of positions within context) - and can this position embedding be more generalized to other methods of storing tokens, instead of just a rly long string - hierarchical - graphical -\n\n\n\nresidual stream: linear combination of inputs and attention layer outputs - it’s entirely linear, and acts as a subspace - attention layers perform projections onto the subspace - due to linearity, at any 2 points, can make a matrix which represents the whole transformation up to that point - ? could this be used in actual transformers to reduce the amount of computation used? - has to be in parts which are linear\n\n\n\n\n\nT(t) : Predicted Logits from input tokens (the Transformer Output) (context x vocab) matrix - “Activation, Privileged Basis”\nt : a vector of tokens (each of which is a one-hot encoded vector) in the context (context x vocab) matrix - “Activation, Privileged Basis”\n\\(x^n\\) : vector of embedding vectors in the residual stream (at layer n) (context x semantic dims) matrix - each vector is the updated semantic representation of a context token after n layers - “Activation, Not Privileged Basis”\n\\(W_E\\) : Embedding Weights, a transformation from one-hot input vectors to embedding vectors (semantic dims x vocab) transformation matrix - input is a vector of tokens (the context), output is a vector of embedding vectors - only takes into account the type of tokens - (frequency?) - Parameter\n\\(W_P\\) : Positional Embedding Weights, a transformation from position in context to embedding vector (semantic dims x context) transformation matrix - input is a vector of tokens (the context), output is a vector of embedding vectors - only takes into account the positions - (time?) - Parameter\n\\(W_U\\) : Unembedding Weights, a transformation from embedding vectors to vocab vectors (logits) (vocab x semantic dims) transformation matrix - (is this the inverse of W_E?) - includes softmax? - Parameter\n\n\n\n\\(H_n\\) : Attention Heads, the set of attention heads at a layer n\n\\(h \\in H_n\\) : An attention head, a function from embedding vectors to embedding vectors\n\\(h(x)\\) : The output of attention head \\(h\\) given some embedding vectors \\(x\\) (context, semantic dims) matrix - a vector of semantic embedding vectors of each token of the context\n\\(\\bf{A}^h\\) : Attention pattern of head h, Not sure what this is, the matrix representing the linear transformation? a transformation from embedded context vectors to embedded context vectors? (context, context) transformation matrix\nFor each head h: this is how you go from input \\(x_i\\) to output \\(h(x)_i\\) Informally: embedding vector -(Value Matrix)-&gt; value vector -(Attention Pattern)-&gt; result vector -(Output Matrix)&gt; output embedding vector Formally: \\(W_V​x_i = v_i\\) , then \\(\\sum_j{ A_{i, j} v_j} = r_i\\) , then \\(W_Or_i=h(x)_i\\)\n\n\n\n\n\n\n“A privileged basis occurs when some aspect of a model’s architecture encourages neural network features to align with basis dimensions”\nConfusion: A privileged basis is when vectors (aka directions) have specific meanings and can’t be rotated without changing those meanings? - but don’t embedding values have meanings assigned to specific directions? unless the meanings are all relative to other vectors - what are the embedding values if their directions don’t have specific meanings - or maybe the directions do have specific meanings, but these meanings don’t have to align to particular neurons\nExamples: “for example (a privileged basis occurs) because of a sparse activation function such as ReLU”\nVectors which do not have a privileged basis: “it doesn’t really make sense to look at the”neurons” (ie. basis dimensions) of activations like the residual stream, keys, queries or values, which don’t have a privileged basis.” - residual stream / embedding - Output of attention head \\(h\\) - Query, key, value and result vectors of attention head \\(h\\) - Output of MLP layer \\(m\\)\nVectors which do have a privileged basis: - Transformer logits - One-hot encoded tokens - Attention pattern of attention head \\(h\\) - Activations of MLP layer \\(m\\)\n\n\n\nwhen combined, is the transformation of the input to output\nvalue matrix is the input token, output is the output token - and when multiplied together, they make up the transformation\nSpeculative what if you find the nearest token (or token sequence) to this final output value vector, to get a description\n\n\n\nFrom a point in embedding space, how to efficiently find a sequence of tokens which approximates that value - maybe given a minimum variance or distance from target - can we even actually represent sequences of tokens as a single point in the embedding space?\nhow to represent an arbitrary sequence of tokens as a single point in an embedding space - what are the scaling difficulties of representing more and more complex stuff in a single embedding space? - relationship of necessary dim size to token sequence length? - like maybe the more tokens in a sequence you’d like to represent, the more embedding dimensions you need to represent the semantic information - maybe instead of using the same embedding dim for everything, use different ones for different “levels” of token sequence length - sort of representing complexity? abstraction?\nconstruct graph relations from - attention to get"
  },
  {
    "objectID": "posts/transformers/transformers.html#things-i-dont-get",
    "href": "posts/transformers/transformers.html#things-i-dont-get",
    "title": "Transformer Notes (Draft)",
    "section": "",
    "text": "privileged basis definition\n“activation”, in the context of activation vs parameter\nhow exactly is position embedded into a vector space (in the space that the key and query vectors are in) - and if it even does encode position (or like subsets of positions within context) - and can this position embedding be more generalized to other methods of storing tokens, instead of just a rly long string - hierarchical - graphical -"
  },
  {
    "objectID": "posts/transformers/transformers.html#notes",
    "href": "posts/transformers/transformers.html#notes",
    "title": "Transformer Notes (Draft)",
    "section": "",
    "text": "residual stream: linear combination of inputs and attention layer outputs - it’s entirely linear, and acts as a subspace - attention layers perform projections onto the subspace - due to linearity, at any 2 points, can make a matrix which represents the whole transformation up to that point - ? could this be used in actual transformers to reduce the amount of computation used? - has to be in parts which are linear"
  },
  {
    "objectID": "posts/transformers/transformers.html#basic-definitions",
    "href": "posts/transformers/transformers.html#basic-definitions",
    "title": "Transformer Notes (Draft)",
    "section": "",
    "text": "T(t) : Predicted Logits from input tokens (the Transformer Output) (context x vocab) matrix - “Activation, Privileged Basis”\nt : a vector of tokens (each of which is a one-hot encoded vector) in the context (context x vocab) matrix - “Activation, Privileged Basis”\n\\(x^n\\) : vector of embedding vectors in the residual stream (at layer n) (context x semantic dims) matrix - each vector is the updated semantic representation of a context token after n layers - “Activation, Not Privileged Basis”\n\\(W_E\\) : Embedding Weights, a transformation from one-hot input vectors to embedding vectors (semantic dims x vocab) transformation matrix - input is a vector of tokens (the context), output is a vector of embedding vectors - only takes into account the type of tokens - (frequency?) - Parameter\n\\(W_P\\) : Positional Embedding Weights, a transformation from position in context to embedding vector (semantic dims x context) transformation matrix - input is a vector of tokens (the context), output is a vector of embedding vectors - only takes into account the positions - (time?) - Parameter\n\\(W_U\\) : Unembedding Weights, a transformation from embedding vectors to vocab vectors (logits) (vocab x semantic dims) transformation matrix - (is this the inverse of W_E?) - includes softmax? - Parameter\n\n\n\n\\(H_n\\) : Attention Heads, the set of attention heads at a layer n\n\\(h \\in H_n\\) : An attention head, a function from embedding vectors to embedding vectors\n\\(h(x)\\) : The output of attention head \\(h\\) given some embedding vectors \\(x\\) (context, semantic dims) matrix - a vector of semantic embedding vectors of each token of the context\n\\(\\bf{A}^h\\) : Attention pattern of head h, Not sure what this is, the matrix representing the linear transformation? a transformation from embedded context vectors to embedded context vectors? (context, context) transformation matrix\nFor each head h: this is how you go from input \\(x_i\\) to output \\(h(x)_i\\) Informally: embedding vector -(Value Matrix)-&gt; value vector -(Attention Pattern)-&gt; result vector -(Output Matrix)&gt; output embedding vector Formally: \\(W_V​x_i = v_i\\) , then \\(\\sum_j{ A_{i, j} v_j} = r_i\\) , then \\(W_Or_i=h(x)_i\\)"
  },
  {
    "objectID": "posts/transformers/transformers.html#notes-1",
    "href": "posts/transformers/transformers.html#notes-1",
    "title": "Transformer Notes (Draft)",
    "section": "",
    "text": "“A privileged basis occurs when some aspect of a model’s architecture encourages neural network features to align with basis dimensions”\nConfusion: A privileged basis is when vectors (aka directions) have specific meanings and can’t be rotated without changing those meanings? - but don’t embedding values have meanings assigned to specific directions? unless the meanings are all relative to other vectors - what are the embedding values if their directions don’t have specific meanings - or maybe the directions do have specific meanings, but these meanings don’t have to align to particular neurons\nExamples: “for example (a privileged basis occurs) because of a sparse activation function such as ReLU”\nVectors which do not have a privileged basis: “it doesn’t really make sense to look at the”neurons” (ie. basis dimensions) of activations like the residual stream, keys, queries or values, which don’t have a privileged basis.” - residual stream / embedding - Output of attention head \\(h\\) - Query, key, value and result vectors of attention head \\(h\\) - Output of MLP layer \\(m\\)\nVectors which do have a privileged basis: - Transformer logits - One-hot encoded tokens - Attention pattern of attention head \\(h\\) - Activations of MLP layer \\(m\\)\n\n\n\nwhen combined, is the transformation of the input to output\nvalue matrix is the input token, output is the output token - and when multiplied together, they make up the transformation\nSpeculative what if you find the nearest token (or token sequence) to this final output value vector, to get a description\n\n\n\nFrom a point in embedding space, how to efficiently find a sequence of tokens which approximates that value - maybe given a minimum variance or distance from target - can we even actually represent sequences of tokens as a single point in the embedding space?\nhow to represent an arbitrary sequence of tokens as a single point in an embedding space - what are the scaling difficulties of representing more and more complex stuff in a single embedding space? - relationship of necessary dim size to token sequence length? - like maybe the more tokens in a sequence you’d like to represent, the more embedding dimensions you need to represent the semantic information - maybe instead of using the same embedding dim for everything, use different ones for different “levels” of token sequence length - sort of representing complexity? abstraction?\nconstruct graph relations from - attention to get"
  },
  {
    "objectID": "posts/Stat365/Matthew-Visualization.html",
    "href": "posts/Stat365/Matthew-Visualization.html",
    "title": "Narrative Visualization",
    "section": "",
    "text": "counts_male &lt;- age_gaps |&gt; \n  filter(character_1_gender == \"man\") |&gt; \n  count(age_difference)\ncounts_female &lt;- age_gaps |&gt; \n  filter(character_1_gender == \"woman\") |&gt; \n  count(age_difference)\n\nggplot() +\n  geom_col(data = counts_male, aes(x = age_difference, y = n), fill = \"steelblue\", width=1) +\n  geom_col(data = counts_female, aes(x = age_difference, y = -n), fill = \"pink\", width=1) +\n  geom_label(aes(x=30, y=40, label=\"Male\"), color=\"steelblue\")+\n  geom_label(aes(x=30, y=-20, label=\"Female\"), color=\"pink\")+\n  labs(title = \"Age Differences in Movie Couples\",\n       subtitle = \"By Gender of the Older Actor\",\n       x='', y='',\n       caption = \"Data sourced from hollywoodagegap.com.\") +\n  theme_minimal()\n\n\n\n\nFigure 1.2 shows the age differences in holywood movie couples. It’s clear that male older relationships are significantly more common than female older relationships, as well as having larger average age gaps.\n\n\n\n\n\nage_gaps &lt;- age_gaps |&gt;\n  mutate(maleFemDiff = case_when(\n    character_1_gender == \"woman\" ~ -1 * age_difference,\n    character_1_gender == \"man\" ~ age_difference\n  ))\n\nggplot() +\n  geom_histogram(data = age_gaps %&gt;% filter(maleFemDiff &gt;= 0), aes(x = maleFemDiff), fill = \"steelblue\", binwidth = 1) +\n  geom_histogram(data = age_gaps %&gt;% filter(maleFemDiff &lt; 0), aes(x = maleFemDiff), fill = \"pink\", binwidth = 1) +\n  geom_label(aes(x =20, y = 35, label = \"Male\"), color = \"steelblue\") +\n  geom_label(aes(x = -20, y = 35, label = \"Female\"), color = \"pink\")+\n    labs(\n    title = \"Age Differences in Heterosexual Movie Couples\",\n    subtitle = \"By age of Male character - Female character\",\n    y=\"Count\",\n    x=\"Male age - Female age\")+\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# normality check:\nqqnorm(age_gaps$maleFemDiff)\nqqline(age_gaps$maleFemDiff)\n\n\n\n\n\n\n\n# t.test\nt.test(age_gaps$maleFemDiff, mu=0, alternative=\"greater\")\n\n\n    One Sample t-test\n\ndata:  age_gaps$maleFemDiff\nt = 27.434, df = 1154, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is greater than 0\n95 percent confidence interval:\n 7.946419      Inf\nsample estimates:\nmean of x \n  8.45368"
  }
]