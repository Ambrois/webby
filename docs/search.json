[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Matthew Huang",
    "section": "",
    "text": "Welcome to my website :P\nClick here to download a virus: TODO add virus"
  },
  {
    "objectID": "Projects.html",
    "href": "Projects.html",
    "title": "Projects",
    "section": "",
    "text": "Transformers\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/about/about.html",
    "href": "posts/about/about.html",
    "title": "About",
    "section": "",
    "text": "Close this page right now."
  },
  {
    "objectID": "posts/about/about.html#about-this-blog",
    "href": "posts/about/about.html#about-this-blog",
    "title": "About",
    "section": "",
    "text": "Close this page right now."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Close this page right now."
  },
  {
    "objectID": "about.html#about-this-blog",
    "href": "about.html#about-this-blog",
    "title": "About",
    "section": "",
    "text": "Close this page right now."
  },
  {
    "objectID": "posts/transformers.html",
    "href": "posts/transformers.html",
    "title": "Transformers",
    "section": "",
    "text": "Transformer Circuits Thread (transformer-circuits.pub)\n\n\nprivileged basis definition\n“activation”, in the context of activation vs parameter\nhow exactly is position embedded into a vector space (in the space that the key and query vectors are in) - and if it even does encode position (or like subsets of positions within context) - and can this position embedding be more generalized to other methods of storing tokens, instead of just a rly long string - hierarchical - graphical -\n\n\n\nresidual stream: linear combination of inputs and attention layer outputs - it’s entirely linear, and acts as a subspace - attention layers perform projections onto the subspace - due to linearity, at any 2 points, can make a matrix which represents the whole transformation up to that point - ? could this be used in actual transformers to reduce the amount of computation used? - has to be in parts which are linear\n\n\n\n\n\nT(t) : Predicted Logits from input tokens (the Transformer Output) (context x vocab) matrix - “Activation, Privileged Basis”\nt : a vector of tokens (each of which is a one-hot encoded vector) in the context (context x vocab) matrix - “Activation, Privileged Basis”\n\\(x^n\\) : vector of embedding vectors in the residual stream (at layer n) (context x semantic dims) matrix - each vector is the updated semantic representation of a context token after n layers - “Activation, Not Privileged Basis”\n\\(W_E\\) : Embedding Weights, a transformation from one-hot input vectors to embedding vectors (semantic dims x vocab) transformation matrix - input is a vector of tokens (the context), output is a vector of embedding vectors - only takes into account the type of tokens - (frequency?) - Parameter\n\\(W_P\\) : Positional Embedding Weights, a transformation from position in context to embedding vector (semantic dims x context) transformation matrix - input is a vector of tokens (the context), output is a vector of embedding vectors - only takes into account the positions - (time?) - Parameter\n\\(W_U\\) : Unembedding Weights, a transformation from embedding vectors to vocab vectors (logits) (vocab x semantic dims) transformation matrix - (is this the inverse of W_E?) - includes softmax? - Parameter\n\n\n\n\\(H_n\\) : Attention Heads, the set of attention heads at a layer n\n\\(h \\in H_n\\) : An attention head, a function from embedding vectors to embedding vectors\n\\(h(x)\\) : The output of attention head \\(h\\) given some embedding vectors \\(x\\) (context, semantic dims) matrix - a vector of semantic embedding vectors of each token of the context\n\\(\\bf{A}^h\\) : Attention pattern of head h, Not sure what this is, the matrix representing the linear transformation? a transformation from embedded context vectors to embedded context vectors? (context, context) transformation matrix\nFor each head h: this is how you go from input \\(x_i\\) to output \\(h(x)_i\\) Informally: embedding vector -(Value Matrix)-&gt; value vector -(Attention Pattern)-&gt; result vector -(Output Matrix)&gt; output embedding vector Formally: \\(W_V​x_i = v_i\\) , then \\(\\sum_j{ A_{i, j} v_j} = r_i\\) , then \\(W_Or_i=h(x)_i\\)\n\n\n\n\n\n\n“A privileged basis occurs when some aspect of a model’s architecture encourages neural network features to align with basis dimensions”\nConfusion: A privileged basis is when vectors (aka directions) have specific meanings and can’t be rotated without changing those meanings? - but don’t embedding values have meanings assigned to specific directions? unless the meanings are all relative to other vectors - what are the embedding values if their directions don’t have specific meanings - or maybe the directions do have specific meanings, but these meanings don’t have to align to particular neurons\nExamples: “for example (a privileged basis occurs) because of a sparse activation function such as ReLU”\nVectors which do not have a privileged basis: “it doesn’t really make sense to look at the”neurons” (ie. basis dimensions) of activations like the residual stream, keys, queries or values, which don’t have a privileged basis.” - residual stream / embedding - Output of attention head \\(h\\) - Query, key, value and result vectors of attention head \\(h\\) - Output of MLP layer \\(m\\)\nVectors which do have a privileged basis: - Transformer logits - One-hot encoded tokens - Attention pattern of attention head \\(h\\) - Activations of MLP layer \\(m\\)\n\n\n\nwhen combined, is the transformation of the input to output\nvalue matrix is the input token, output is the output token - and when multiplied together, they make up the transformation\nSpeculative what if you find the nearest token (or token sequence) to this final output value vector, to get a description\n\n\n\nFrom a point in embedding space, how to efficiently find a sequence of tokens which approximates that value - maybe given a minimum variance or distance from target - can we even actually represent sequences of tokens as a single point in the embedding space?\nhow to represent an arbitrary sequence of tokens as a single point in an embedding space - what are the scaling difficulties of representing more and more complex stuff in a single embedding space? - relationship of necessary dim size to token sequence length? - like maybe the more tokens in a sequence you’d like to represent, the more embedding dimensions you need to represent the semantic information - maybe instead of using the same embedding dim for everything, use different ones for different “levels” of token sequence length - sort of representing complexity? abstraction?\nconstruct graph relations from - attention to get"
  },
  {
    "objectID": "posts/transformers.html#things-i-dont-get",
    "href": "posts/transformers.html#things-i-dont-get",
    "title": "Transformers",
    "section": "",
    "text": "privileged basis definition\n“activation”, in the context of activation vs parameter\nhow exactly is position embedded into a vector space (in the space that the key and query vectors are in) - and if it even does encode position (or like subsets of positions within context) - and can this position embedding be more generalized to other methods of storing tokens, instead of just a rly long string - hierarchical - graphical -"
  },
  {
    "objectID": "posts/transformers.html#notes",
    "href": "posts/transformers.html#notes",
    "title": "Transformers",
    "section": "",
    "text": "residual stream: linear combination of inputs and attention layer outputs - it’s entirely linear, and acts as a subspace - attention layers perform projections onto the subspace - due to linearity, at any 2 points, can make a matrix which represents the whole transformation up to that point - ? could this be used in actual transformers to reduce the amount of computation used? - has to be in parts which are linear"
  },
  {
    "objectID": "posts/transformers.html#basic-definitions",
    "href": "posts/transformers.html#basic-definitions",
    "title": "Transformers",
    "section": "",
    "text": "T(t) : Predicted Logits from input tokens (the Transformer Output) (context x vocab) matrix - “Activation, Privileged Basis”\nt : a vector of tokens (each of which is a one-hot encoded vector) in the context (context x vocab) matrix - “Activation, Privileged Basis”\n\\(x^n\\) : vector of embedding vectors in the residual stream (at layer n) (context x semantic dims) matrix - each vector is the updated semantic representation of a context token after n layers - “Activation, Not Privileged Basis”\n\\(W_E\\) : Embedding Weights, a transformation from one-hot input vectors to embedding vectors (semantic dims x vocab) transformation matrix - input is a vector of tokens (the context), output is a vector of embedding vectors - only takes into account the type of tokens - (frequency?) - Parameter\n\\(W_P\\) : Positional Embedding Weights, a transformation from position in context to embedding vector (semantic dims x context) transformation matrix - input is a vector of tokens (the context), output is a vector of embedding vectors - only takes into account the positions - (time?) - Parameter\n\\(W_U\\) : Unembedding Weights, a transformation from embedding vectors to vocab vectors (logits) (vocab x semantic dims) transformation matrix - (is this the inverse of W_E?) - includes softmax? - Parameter\n\n\n\n\\(H_n\\) : Attention Heads, the set of attention heads at a layer n\n\\(h \\in H_n\\) : An attention head, a function from embedding vectors to embedding vectors\n\\(h(x)\\) : The output of attention head \\(h\\) given some embedding vectors \\(x\\) (context, semantic dims) matrix - a vector of semantic embedding vectors of each token of the context\n\\(\\bf{A}^h\\) : Attention pattern of head h, Not sure what this is, the matrix representing the linear transformation? a transformation from embedded context vectors to embedded context vectors? (context, context) transformation matrix\nFor each head h: this is how you go from input \\(x_i\\) to output \\(h(x)_i\\) Informally: embedding vector -(Value Matrix)-&gt; value vector -(Attention Pattern)-&gt; result vector -(Output Matrix)&gt; output embedding vector Formally: \\(W_V​x_i = v_i\\) , then \\(\\sum_j{ A_{i, j} v_j} = r_i\\) , then \\(W_Or_i=h(x)_i\\)"
  },
  {
    "objectID": "posts/transformers.html#notes-1",
    "href": "posts/transformers.html#notes-1",
    "title": "Transformers",
    "section": "",
    "text": "“A privileged basis occurs when some aspect of a model’s architecture encourages neural network features to align with basis dimensions”\nConfusion: A privileged basis is when vectors (aka directions) have specific meanings and can’t be rotated without changing those meanings? - but don’t embedding values have meanings assigned to specific directions? unless the meanings are all relative to other vectors - what are the embedding values if their directions don’t have specific meanings - or maybe the directions do have specific meanings, but these meanings don’t have to align to particular neurons\nExamples: “for example (a privileged basis occurs) because of a sparse activation function such as ReLU”\nVectors which do not have a privileged basis: “it doesn’t really make sense to look at the”neurons” (ie. basis dimensions) of activations like the residual stream, keys, queries or values, which don’t have a privileged basis.” - residual stream / embedding - Output of attention head \\(h\\) - Query, key, value and result vectors of attention head \\(h\\) - Output of MLP layer \\(m\\)\nVectors which do have a privileged basis: - Transformer logits - One-hot encoded tokens - Attention pattern of attention head \\(h\\) - Activations of MLP layer \\(m\\)\n\n\n\nwhen combined, is the transformation of the input to output\nvalue matrix is the input token, output is the output token - and when multiplied together, they make up the transformation\nSpeculative what if you find the nearest token (or token sequence) to this final output value vector, to get a description\n\n\n\nFrom a point in embedding space, how to efficiently find a sequence of tokens which approximates that value - maybe given a minimum variance or distance from target - can we even actually represent sequences of tokens as a single point in the embedding space?\nhow to represent an arbitrary sequence of tokens as a single point in an embedding space - what are the scaling difficulties of representing more and more complex stuff in a single embedding space? - relationship of necessary dim size to token sequence length? - like maybe the more tokens in a sequence you’d like to represent, the more embedding dimensions you need to represent the semantic information - maybe instead of using the same embedding dim for everything, use different ones for different “levels” of token sequence length - sort of representing complexity? abstraction?\nconstruct graph relations from - attention to get"
  },
  {
    "objectID": "Blog.html",
    "href": "Blog.html",
    "title": "Projects",
    "section": "",
    "text": "Transformers\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  }
]